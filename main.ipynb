{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrVJuplcc_Pt"
      },
      "source": [
        "## Collaborative Filtering Assignment 1\n",
        "### Name: Shaney Waris\n",
        "### Roll no.: 2018308"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quZ731utc_P0"
      },
      "source": [
        "# I have used these 4 libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuX0hn9fc_P1"
      },
      "source": [
        "# Training Dataset.\n",
        "u1Base_df = pd.read_csv('./Dataset/ml-100k/u1.base', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u2Base_df = pd.read_csv('./Dataset/ml-100k/u2.base', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u3Base_df = pd.read_csv('./Dataset/ml-100k/u3.base', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u4Base_df = pd.read_csv('./Dataset/ml-100k/u3.base', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u5Base_df = pd.read_csv('./Dataset/ml-100k/u4.base', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcncpNGic_P2"
      },
      "source": [
        "# Testing Dataset.\n",
        "u1Test_df = pd.read_csv('./Dataset/ml-100k/u1.test', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u2Test_df = pd.read_csv('./Dataset/ml-100k/u2.test', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u3Test_df = pd.read_csv('./Dataset/ml-100k/u3.test', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u4Test_df = pd.read_csv('./Dataset/ml-100k/u4.test', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])\n",
        "u5Test_df = pd.read_csv('./Dataset/ml-100k/u5.test', sep='\\t', names=['userId', 'itemId', 'Rating', 'Timestamp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4B_eJHac_P2"
      },
      "source": [
        "## User Based Collaborative Filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "runu5JQCc_P2"
      },
      "source": [
        "# Storing all the training & testing dataframes in a list. \n",
        "Train_df = [u1Base_df, u2Base_df, u3Base_df, u4Base_df, u5Base_df]\n",
        "Test_df = [u1Test_df, u2Test_df, u3Test_df, u4Test_df, u5Test_df]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqe1lunUc_P3"
      },
      "source": [
        "def predict_rating_UB(userId, itemId, train_pt_df, sm_df, tau):\n",
        "    users_similarities = sm_df[userId]   # cosine similarities of user 'userId' with all other users.\n",
        "    # Check if a completly new movie comes in my testing dataset.\n",
        "    try:\n",
        "        users_ratings = train_pt_df[itemId]  # ratings of all users for item 'itemId'\n",
        "    except:\n",
        "        return -1   # That movie doesn't exit in my training dataset. Ignore this case.\n",
        "    \n",
        "    # Not consider users with similarities less than threshold tau. (Here, similarity of the same user will also not considered.)\n",
        "    drop_indices = users_similarities[users_similarities < tau].index\n",
        "    users_ratings = users_ratings.drop(drop_indices)\n",
        "    users_similarities = users_similarities.drop(drop_indices)\n",
        "    \n",
        "    # Not consider users who haven't rated the movie 'itemId'\n",
        "    drop_indices = users_ratings[users_ratings == 0].index\n",
        "    users_ratings = users_ratings.drop(drop_indices)\n",
        "    users_similarities = users_similarities.drop(drop_indices)\n",
        "    \n",
        "    global coverage\n",
        "    # If I encountered with coverage problem i.e, no similar users exist after threshold. Then take aveage rating of the movie.\n",
        "    if len(users_similarities) == 0 or len(users_ratings) == 0:\n",
        "        coverage = coverage + 1\n",
        "        l = [x for x in list(train_pt_df[itemId]) if x != 0]\n",
        "        return sum(l)/len(l)\n",
        "    else:\n",
        "        # Calculate weighted ratings \n",
        "        weighted_ratings = np.dot(users_ratings, users_similarities)\n",
        "\n",
        "        # Normalized rating by sum of all the similarities. \n",
        "        users_similarities_sum = users_similarities.sum()\n",
        "\n",
        "        # return the predicted rating.\n",
        "        return weighted_ratings/users_similarities_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPttk_b2c_P4"
      },
      "source": [
        "# This function takes the training & testing dataframes and return the MAE.\n",
        "def UB_MAE(train_df, test_df, tau):\n",
        "    # pivot the training dataframe.\n",
        "    train_pt_df = pd.pivot_table(train_df, values='Rating', index='userId', columns='itemId')\n",
        "    # Replace the NA values with 0   (Note: I observed -> No user in the whole dataset have rated 0 to any movie)\n",
        "    train_pt_df = train_pt_df.fillna(0)\n",
        "    # Calculate the cosine similarities of user-user.\n",
        "    sm_df = pd.DataFrame(cosine_similarity(train_pt_df), index=train_pt_df.index, columns=train_pt_df.index)\n",
        "    # Not consider the similarity of same user while predicting the rating. eg: similarity of user 1 with user 1.\n",
        "    np.fill_diagonal(sm_df.values, 0)\n",
        "    \n",
        "    # Actual Ratings of testing dataframe.\n",
        "    actual_ratings  = test_df['Rating']\n",
        "    # Start predicting the ratings of testing dataframe.\n",
        "    predicted_ratings = []\n",
        "    for userId, itemId in zip(test_df['userId'], test_df['itemId']):\n",
        "        predicted_ratings.append(predict_rating_UB(userId, itemId, train_pt_df, sm_df, tau))\n",
        "    \n",
        "    # Ignore the case when predicted rating is -1. (Because no such users are available to predict the rating. #Coverage_Problem)\n",
        "    new_actual_ratings = []\n",
        "    new_predicted_ratings = []\n",
        "    for i in range(0, len(predicted_ratings)):\n",
        "        if predicted_ratings[i] == -1:\n",
        "            continue\n",
        "        else:\n",
        "            new_actual_ratings.append(actual_ratings[i])\n",
        "            new_predicted_ratings.append(predicted_ratings[i])\n",
        "\n",
        "    # return the MAE between Actual Ratings & Predicted Ratings.\n",
        "    return mean_absolute_error(new_actual_ratings, new_predicted_ratings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1v9q4vc_P4",
        "outputId": "93c35950-245f-4262-9208-e92be1495bc9"
      },
      "source": [
        "# For all the threshold values.\n",
        "for tau in [0.4, 0.5, 0.6, 0.7]:\n",
        "    print(\"\\n*** For tau =\", tau, \"***\")\n",
        "    cross_validation = []\n",
        "    # 5 fold Cross Validation.\n",
        "    for i in range(0, 5):\n",
        "        coverage = 0\n",
        "        trainDataset = Train_df[i] # 1 training dataframe.\n",
        "        testDataset = Test_df[i]   # 1 testing dataframe.\n",
        "        mae_value = UB_MAE(trainDataset, testDataset, tau)  # Returning MAE for each fold.\n",
        "        cross_validation.append(mae_value)\n",
        "        print(\"FOLD \" + str(i+1) + \": Taking u\"+ str(i+1) + \".test as Testing DataSet, MAE =\", mae_value, \", Coverage = \" + str(100-((coverage*100)/20000)) + \"%\")\n",
        "    print(\"MEAN Value =\", sum(cross_validation)/5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** For tau = 0.4 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.8475006447582307 , Coverage = 48.655%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.8478761926090771 , Coverage = 63.47%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.8397190856313358 , Coverage = 61.255%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.8203078252670899 , Coverage = 68.91%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.8186155541110084 , Coverage = 61.67%\n",
            "MEAN Value = 0.8348038604753484\n",
            "\n",
            "*** For tau = 0.5 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.836024471030855 , Coverage = 7.969999999999999%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.8345119280063997 , Coverage = 15.825000000000003%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.8266754201144628 , Coverage = 16.810000000000002%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.8105526572396692 , Coverage = 24.200000000000003%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.8096246972385351 , Coverage = 17.0%\n",
            "MEAN Value = 0.8234778347259842\n",
            "\n",
            "*** For tau = 0.6 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.8256316310412225 , Coverage = 0.2950000000000017%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.8185844766963448 , Coverage = 0.9249999999999972%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.8095313805227538 , Coverage = 0.6749999999999972%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.7918141995985887 , Coverage = 0.8799999999999955%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7953889430830614 , Coverage = 1.0900000000000034%\n",
            "MEAN Value = 0.8081901261883943\n",
            "\n",
            "*** For tau = 0.7 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.8261415793915304 , Coverage = 0.18000000000000682%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.8183431468757628 , Coverage = 0.4099999999999966%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.8103010226084848 , Coverage = 0.20999999999999375%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.7928346899239228 , Coverage = 0.05500000000000682%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7944856462231144 , Coverage = 0.0%\n",
            "MEAN Value = 0.808421217004563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrX4svIVc_P6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Zy9Hxic_P9"
      },
      "source": [
        "## Item Based Collaborative Filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcNIfnddc_P9"
      },
      "source": [
        "def predict_rating_IB(userId, itemId, train_pt_df, sm_df, K):\n",
        "    try:\n",
        "        items_similarities = sm_df[itemId]  # cosine similarities of item 'itemId' with all other items.\n",
        "    except:\n",
        "        return -1\n",
        "    users_ratings = train_pt_df[userId]  # ratings of all items for user 'userId'\n",
        "    \n",
        "    # Consider only highest K item similarities. (Here, similarity of the same item will also not considered).\n",
        "    d = dict(items_similarities) # {itemId : item_similarity}\n",
        "    d_sorted = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
        "    drop_rest_users = list(d_sorted.keys())\n",
        "    drop_rest_users = drop_rest_users[K:]\n",
        "    users_ratings = users_ratings.drop(drop_rest_users)\n",
        "    items_similarities = items_similarities.drop(drop_rest_users)\n",
        "    \n",
        "    # Not consider items whose ratings are NA.\n",
        "    drop_empty_ratings = users_ratings[users_ratings == 0].index\n",
        "    users_ratings = users_ratings.drop(drop_empty_ratings)\n",
        "    items_similarities = items_similarities.drop(drop_empty_ratings)\n",
        "    \n",
        "    global coverage\n",
        "    # # If I encountered with coverage problem i.e, no similar items exist after threshold. Then take aveage rating of the movie.\n",
        "    if len(users_ratings) == 0 or len(items_similarities) == 0:\n",
        "        coverage = coverage + 1\n",
        "        ll = [x for x in list(train_pt_df.T[itemId]) if x != 0]\n",
        "        return sum(ll)/len(ll)\n",
        "    else:\n",
        "        # Normalize all the similarities of all the items.\n",
        "        items_similarities_sum = sum(items_similarities)\n",
        "        items_similarities = [i/items_similarities_sum for i in items_similarities]\n",
        "\n",
        "        # Linearly interpolate the Active User 'userId' rated items by corresponding normalized similarities.\n",
        "        linear_interpolation = np.dot(users_ratings, items_similarities)\n",
        "\n",
        "        # return this predicted rating.\n",
        "        return linear_interpolation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4us3A54c_P9"
      },
      "source": [
        "# This function takes the training & testing dataframes and return the MAE.\n",
        "def IB_MAE(train_df, test_df, K):\n",
        "    # pivot the training dataframe and Transpose it.\n",
        "    train_pt_df = pd.pivot_table(train_df, values='Rating', index='userId', columns='itemId').T\n",
        "    # Replace the NA values with 0   (Note: I observed -> No user in the whole dataset have rated 0 to any movie)\n",
        "    train_pt_df = train_pt_df.fillna(0)\n",
        "    # Calculate the cosine similarities of item-item.\n",
        "    sm_df = pd.DataFrame(cosine_similarity(train_pt_df), index=train_pt_df.index, columns=train_pt_df.index)\n",
        "    # Not consider the similarity of same item while predicting the rating. eg: similarity of item 1 with item 1.\n",
        "    np.fill_diagonal(sm_df.values, 0)\n",
        "    \n",
        "    # Actual Ratings of testing dataframe.\n",
        "    actual_ratings  = test_df['Rating']\n",
        "    # Start predicting the ratings of testing dataframe.\n",
        "    predicted_ratings = []\n",
        "    for userId, itemId in (zip(test_df['userId'], test_df['itemId'])):\n",
        "        predicted_ratings.append(predict_rating_IB(userId, itemId, train_pt_df, sm_df, K))\n",
        "        \n",
        "    # Ignore the case when predicted rating is -1. (Because no such items are available to predict the rating. #Coverage_Problem)\n",
        "    new_actual_ratings = []\n",
        "    new_predicted_ratings = []\n",
        "    for i in range(0, len(predicted_ratings)):\n",
        "        if predicted_ratings[i] == -1:\n",
        "            continue\n",
        "        else:\n",
        "            new_actual_ratings.append(actual_ratings[i])\n",
        "            new_predicted_ratings.append(predicted_ratings[i])\n",
        "    \n",
        "    # return the MAE between Actual Ratings & Predicted Ratings.\n",
        "    return mean_absolute_error(new_actual_ratings, new_predicted_ratings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5rIzNscc_P-",
        "outputId": "076f2a40-56d3-4d0a-9bcb-a1fd8e898535"
      },
      "source": [
        "# For all the threshold values.\n",
        "for K in [10, 20, 30, 40]:\n",
        "    print(\"\\n*** For K =\", K, \"***\")\n",
        "    cross_validation = []\n",
        "    # 5 fold Cross Validation.\n",
        "    for i in range(0, 5):\n",
        "        coverage = 0\n",
        "        trainDataset = Train_df[i]  # 1 training dataframe.\n",
        "        testDataset = Test_df[i]    # 1 testing dataframe.\n",
        "        mae_value = IB_MAE(trainDataset, testDataset, K)   # Returning MAE for each fold.\n",
        "        cross_validation.append(mae_value)\n",
        "        print(\"FOLD \" + str(i+1) + \": Taking u\"+ str(i+1) + \".test as Testing DataSet, MAE =\", mae_value, \", Coverage = \" + str(100-((coverage*100)/20000)) + \"%\")\n",
        "    print(\"MEAN Value =\", sum(cross_validation)/5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** For K = 10 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.8065361949096769 , Coverage = 88.09%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.7939964674859856 , Coverage = 90.93%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.7797270876387902 , Coverage = 91.13%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.7696899678196355 , Coverage = 95.45%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7666360317959755 , Coverage = 94.355%\n",
            "MEAN Value = 0.7833171499300129\n",
            "\n",
            "*** For K = 20 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.7884116848570262 , Coverage = 94.475%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.7699057629281751 , Coverage = 95.77%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.7681496007174117 , Coverage = 95.99%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.760143479878848 , Coverage = 98.275%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7529098809777055 , Coverage = 97.965%\n",
            "MEAN Value = 0.7679040818718332\n",
            "\n",
            "*** For K = 30 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.7831470866249407 , Coverage = 96.7%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.7618879943222987 , Coverage = 97.37%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.7619395582626856 , Coverage = 97.635%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.7578787448650866 , Coverage = 99.055%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7482164840226724 , Coverage = 98.835%\n",
            "MEAN Value = 0.7626139736195369\n",
            "\n",
            "*** For K = 40 ***\n",
            "FOLD 1: Taking u1.test as Testing DataSet, MAE = 0.7791024069436101 , Coverage = 97.825%\n",
            "FOLD 2: Taking u2.test as Testing DataSet, MAE = 0.7614343040790187 , Coverage = 98.34%\n",
            "FOLD 3: Taking u3.test as Testing DataSet, MAE = 0.7581520186952564 , Coverage = 98.49%\n",
            "FOLD 4: Taking u4.test as Testing DataSet, MAE = 0.757736183303115 , Coverage = 99.435%\n",
            "FOLD 5: Taking u5.test as Testing DataSet, MAE = 0.7494023824822783 , Coverage = 99.345%\n",
            "MEAN Value = 0.7611654591006556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOZa-Cc-c_P_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06QkCYiZc_P_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}